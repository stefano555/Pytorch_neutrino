{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet-50 with Neutrino Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing essential libraries\n",
    "this will be cleaned, some are inessential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from torchvision.models.resnet import ResNet, BasicBlock, Bottleneck\n",
    "from torchvision.datasets import MNIST\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import inspect\n",
    "import time\n",
    "from torch import nn, optim\n",
    "import torch\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I check here if cuda gpu is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, Train, and Test\n",
    "\n",
    "I define here the model (ResNet-50) and the train and test function. The model takes as input images channel first with 3 channels. The test definition is week and it could lead to a memory leak. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_list and target_list are two lists of cuda tensors containing respectively the predictions and the \n",
    "#target values. Each tensor will be test_batch_size in lenght. Each list will contain \n",
    "#(amount of test samples/test_batch_size)*epochs samples. E.g. with 10k test samples, test_batch_size = 1k and\n",
    "#4 epochs you'll get a list of 40 elements, each element 1k. Further down these lists will be converted in arrays.\n",
    "\n",
    "pred_list = []\n",
    "target_list = []\n",
    "accuracy = []\n",
    "loss = []\n",
    "epochs = 1\n",
    "test_batch = 1\n",
    "    \n",
    "class MnistResNet(ResNet):\n",
    "    def __init__(self):\n",
    "        super(MnistResNet, self).__init__(Bottleneck, [3,4,6,3], num_classes=3)\n",
    "        self.conv1 = torch.nn.Conv2d(3, 64, \n",
    "            kernel_size=(7, 7), \n",
    "            stride=(2, 2), \n",
    "            padding=(3, 3), bias=False)\n",
    "######if you then use nll_loss you have to use log_sotmax        \n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(\n",
    "            super(MnistResNet, self).forward(x), dim=-1)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            volatile=True\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            pred_list.append(pred)\n",
    "            target_list.append(target)\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    accuracy.append(100. * correct / len(test_loader.dataset))\n",
    "    loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "I implemented here a simple dataset. At the moment, it takes a tensor as data and numpy array as target. Moreover, it only takes one file. This is something that I will need to change in the future since we have several files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing DataSet\n",
    "\n",
    "class NeutrinoDataset(Dataset):\n",
    "    \"\"\"The training table dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, x_tensor, y_path):\n",
    "        #x_filenames = glob(x_path + '*.png') # Get the filenames of all training images\n",
    "        \n",
    "        #self.x_data = [torch.from_numpy(misc.imread(filename)) for filename in x_filenames] # Load the images into torch tensors\n",
    "        #self.y_data = target_label_list # Class labels\n",
    "        self.x_data = x_tensor\n",
    "        self.y_data = torch.from_numpy(np.load(y_path))\n",
    "        self.len = len(self.x_data)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing files and preparing the dataset\n",
    "\n",
    "In this part I import the train and test files, I normalize them and I reshape them to be channel first. This part could also lead to potential memory leaks. It would be nice to convert and normalise the numpy array in one go, but at the moment I can't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 0 254\n",
      "After: 0.0 0.99607843\n",
      "Test Data Shape after conversion to float\n",
      "200\n",
      "200\n",
      "input_shape (3, 200, 200)\n",
      "x_train[0].shape[0] 3\n",
      "x_train[0].shape[1] 200\n",
      "x_train[0].shape[2] 200\n"
     ]
    }
   ],
   "source": [
    "x_train_temp = np.load('/home/stefano/neutrino/data/final_array_x/x_train_numu_500_0.npy')\n",
    "x_test_temp = np.load('/home/stefano/neutrino/data/final_array_x/x_train_numu_500_1.npy')\n",
    "y_test = np.load('/home/stefano/neutrino/data/final_array_y/y_train_numu_500_event_1.npy')\n",
    "y_test = np.load('/home/stefano/neutrino/data/final_array_y/y_train_numu_500_event_1.npy')\n",
    "    \n",
    "y_train_path = '/home/stefano/neutrino/data/final_array_y/y_train_numu_500_event_1.npy'  \n",
    "y_test_path = '/home/stefano/neutrino/data/final_array_y/y_train_numu_500_event_1.npy'\n",
    "\n",
    "\n",
    "\n",
    "#normalise the data\n",
    "old_min = np.min(x_train_temp)\n",
    "old_max = np.max(x_train_temp)\n",
    "print(\"Before:\", old_min, old_max)\n",
    "x_train = x_train_temp.astype('float32')\n",
    "x_test = x_test_temp.astype('float32')\n",
    "x_train /=255\n",
    "x_test /= 255\n",
    "new_min = np.min(x_train)\n",
    "new_max = np.max(x_train)\n",
    "print(\"After:\", new_min, new_max)\n",
    "\n",
    "print(\"Test Data Shape after conversion to float\")\n",
    "    \n",
    "    #read the dimensions from one example in the trainig set\n",
    "img_rows, img_cols = x_train[0].shape[0], x_train[0].shape[1]\n",
    "print(x_train[0].shape[0])\n",
    "print(x_train[0].shape[1])\n",
    "\n",
    "#read the dimensions from one example in the trainig set\n",
    "img_rows, img_cols = x_train[0].shape[0], x_train[0].shape[1]\n",
    "\n",
    "#Different NN libraries (e.g., TF) use different ordering of dimensions\n",
    "#Here we set the \"input shape\" so that later the NN knows what shape to expect\n",
    "x_train_reshaped = x_train.reshape(x_train.shape[0], 3, img_rows, img_cols)\n",
    "x_test_reshaped = x_test.reshape(x_test.shape[0], 3, img_rows, img_cols)\n",
    "input_shape = (3, img_rows, img_cols)\n",
    "\n",
    "print(\"input_shape\",input_shape)\n",
    "print(\"x_train[0].shape[0]\", x_train_reshaped[0].shape[0])\n",
    "print(\"x_train[0].shape[1]\", x_train_reshaped[0].shape[1])\n",
    "print(\"x_train[0].shape[2]\", x_train_reshaped[0].shape[2])\n",
    "\n",
    "x_train_temp = torch.from_numpy(x_train_reshaped).float()\n",
    "x_test_temp = torch.from_numpy(x_test_reshaped).float()\n",
    "\n",
    "dataset_test = NeutrinoDataset(x_test_temp, y_test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of main function\n",
    "\n",
    "I define here the dataset and the main function. This seems to be the stronger part of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "def main():\n",
    "    args = easydict.EasyDict({\n",
    "        \"batch_size\": 1,#it was 64\n",
    "        \"test_batch_size\": 500,\n",
    "        \"epochs\": epochs,\n",
    "        \"lr\": 1.0,\n",
    "        \"gamma\": 0.7,\n",
    "        \"no_cuda\": False,\n",
    "        \"seed\": 1,\n",
    "        \"log_interval\":10,\n",
    "        \"save_model\": False\n",
    "    })\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    #device = torch.device(\"cpu\")\n",
    "    \n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    \n",
    "\n",
    "    \n",
    "    transform_train=transforms.Compose([ToTensor(),transforms.Normalize((0.011209016666666667,), (0.36950463408735745,))])\n",
    "    transform_test=transforms.Compose([ToTensor(),transforms.Normalize((0.0171563,), (0.5148277654099112,))]) \n",
    "    \n",
    "   # x_train_tensor = transform_train(x_train)\n",
    "    #x_test_tensor = transform_test(x_test)\n",
    "    \n",
    "    \n",
    "    #x_train_tensor = transforms.Normalize((0.011209016666666667,), (0.36950463408735745,), x_train_temp)\n",
    "    #x_test_tensor = transforms.Normalize((0.0171563,), (0.5148277654099112,), x_test_temp)\n",
    "    \n",
    "    dataset_train = NeutrinoDataset(x_train_temp, y_train_path)\n",
    "    dataset_test = NeutrinoDataset(x_test_temp, y_test_path)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(dataset=dataset_train,\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset_test,\n",
    "        batch_size=args.test_batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    model = MnistResNet().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(args, model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting your Train and Test Dataset\n",
    "It is important to always check your dataset. With the following lines images from your train and test dataset and randomly selected and plotted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part I calcolate mean and std of the pics in case I find a way to use tensor.normalise. Skip this part when run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda =  torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "%matplotlib inline \n",
    "\n",
    "x_train = torch.from_numpy(np.load('/home/stefano/neutrino/data/final_array_x/x_train_numu_500_0.npy')).float().to(device)\n",
    "x_test = torch.from_numpy(np.load('/home/stefano/neutrino/data/final_array_x/x_train_numu_500_1.npy')).float().to(device)\n",
    "y_train = torch.from_numpy(np.load('/home/stefano/neutrino/data/final_array_y/y_train_numu_500_event_0.npy')).float().to(device)\n",
    "y_test = torch.from_numpy(np.load('/home/stefano/neutrino/data/final_array_y/y_train_numu_500_event_1.npy')).float().to(device)\n",
    "print(x_train.shape)\n",
    "#print(dataset1)\n",
    "#dataset1 = torch.from_numpy(train).float().to(device)\n",
    "#dataset2 = torch.from_numpy(test).float().to(device)\n",
    "#dataset1 = ('../data/final_array_x/x_train_numu_500_0.npy',transforms.Compose([ \n",
    "                           #transforms.ToTensor()\n",
    "                      #]))\n",
    "#dataset2 = ('../data/final_array_x/x_train_numu_500_1.npy',transforms.Compose([ \n",
    "                          # transforms.ToTensor()\n",
    "                       #]))\n",
    "###this part is to calculate mean and std of the dataset resized\n",
    "train_array = np.load('/home/stefano/neutrino/data/final_array_x/x_train_numu_500_0.npy')\n",
    "test_array = np.load('/home/stefano/neutrino/data/final_array_x/x_train_numu_500_1.npy')\n",
    "#for y in range (0,500):\n",
    "#    z, _ = dataset1[y]\n",
    "#    train_array.append(z.numpy()[0])\n",
    "    \n",
    "#for i in range (0,500):\n",
    "#    x, _ = dataset2[i]\n",
    "#    test_array.append(x.numpy()[0])\n",
    "\n",
    "#for i in test_array : \n",
    "#    train_array.append(i)\n",
    "    \n",
    "#total_array=np.array(train_array)\n",
    "\n",
    "print(test_array.mean())\n",
    "print(test_array.std())\n",
    "print(train_array.mean())\n",
    "print(train_array.std())\n",
    "\n",
    "#test mean with size 200x200 is 0.0171563\n",
    "#test std with size 200x200 is 0.5148277654099112\n",
    "#train mean with size 200x200 is 0.011209016666666667\n",
    "#train std with size 200x200 is 0.36950463408735745\n",
    "###############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "\n",
    "The following two cells contain a cool tool to visualise train and test dataset. Useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import zlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import zlib\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "figsize = [14, 20]\n",
    "starting_number = torch.IntTensor.item(torch.LongTensor(1).random_(0, 500-1))#-(nrows*ncols)))\n",
    "#number_pics = dataset1.shape[0]\n",
    "print(x_train.shape)\n",
    "#print('number pics is ', number_pics)\n",
    "print(x_train.shape[1])\n",
    "print(x_train.shape[2])\n",
    "print(x_train.shape[3])\n",
    "\n",
    "print(x_train[1,1,1,1])\n",
    "\n",
    "x_train_cpu=x_train.cpu()\n",
    "y_train_cpu=y_train.cpu()\n",
    "\n",
    "print(\"now showing image \",(starting_number))\n",
    "\n",
    "####this part is for the true value\n",
    "#0 is for NC events\n",
    "#1 is for numu CC events\n",
    "#2 is for nue CC events\n",
    "truth=y_train_cpu[starting_number]\n",
    "if truth==0:\n",
    "    truth_stg=\"NC\"\n",
    "elif truth==1:\n",
    "    truth_stg=\"numu CC\"\n",
    "elif truth==2:\n",
    "    truth_stg=\"nue CC\"\n",
    "\n",
    "a = x_train_cpu[(starting_number),:,:,0]\n",
    "\n",
    "b = x_train_cpu[(starting_number),:,:,1]\n",
    "\n",
    "c = x_train_cpu[(starting_number),:,:,2]\n",
    "\n",
    "\n",
    "f, axarr = plt.subplots(1,3,figsize=figsize)#, constrained_layout=True)\n",
    "f.suptitle('Event Seen From Wire Planes - 200 x 200 pixels - this is a '+truth_stg+' event')\n",
    "im=axarr[0].imshow(a, norm=matplotlib.colors.LogNorm())\n",
    "im2=axarr[1].imshow(b, norm=matplotlib.colors.LogNorm())\n",
    "im3=axarr[2].imshow(c, norm=matplotlib.colors.LogNorm())\n",
    "axarr[0].set_title('U Plane')\n",
    "axarr[1].set_title('V Plane')\n",
    "axarr[2].set_title('W Plane')\n",
    "bar1=f.colorbar(im,ax=axarr[0],fraction=0.046,pad=0.04)\n",
    "bar2=f.colorbar(im2,ax=axarr[1],fraction=0.046,pad=0.04)\n",
    "bar3=f.colorbar(im3,ax=axarr[2],fraction=0.046,pad=0.04)\n",
    "bar1.set_label('Energy [MeV]')\n",
    "bar2.set_label('Energy [MeV]')\n",
    "bar3.set_label('Energy [MeV]')\n",
    "f.tight_layout() #this is to add more space between plots\n",
    "for ax in axarr.flat:\n",
    "    ax.set(xlabel='Wire Plane', ylabel='Time')\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = [14, 20]\n",
    "starting_number = torch.IntTensor.item(torch.LongTensor(1).random_(0, 500-1))#-(nrows*ncols)))\n",
    "#number_pics = dataset1.shape[0]\n",
    "print(x_test.shape)\n",
    "#print('number pics is ', number_pics)\n",
    "print(x_test.shape[1])\n",
    "print(x_test.shape[2])\n",
    "print(x_test.shape[3])\n",
    "\n",
    "print(x_test[1,1,1,1])\n",
    "\n",
    "x_test_cpu=x_test.cpu()\n",
    "y_test_cpu=y_test.cpu()\n",
    "\n",
    "print(\"now showing image \",(starting_number))\n",
    "\n",
    "####this part is for the true value\n",
    "#0 is for NC events\n",
    "#1 is for numu CC events\n",
    "#2 is for nue CC events\n",
    "truth=y_test_cpu[starting_number]\n",
    "if truth==0:\n",
    "    truth_stg=\"NC\"\n",
    "elif truth==1:\n",
    "    truth_stg=\"numu CC\"\n",
    "elif truth==2:\n",
    "    truth_stg=\"nue CC\"\n",
    "\n",
    "a = x_test_cpu[(starting_number),:,:,0]\n",
    "\n",
    "b = x_test_cpu[(starting_number),:,:,1]\n",
    "\n",
    "c = x_test_cpu[(starting_number),:,:,2]\n",
    "\n",
    "\n",
    "f, axarr = plt.subplots(1,3,figsize=figsize)#, constrained_layout=True)\n",
    "f.suptitle('Event Seen From Wire Planes - 200 x 200 pixels - this is a '+truth_stg+' event')\n",
    "im=axarr[0].imshow(a, norm=matplotlib.colors.LogNorm())\n",
    "im2=axarr[1].imshow(b, norm=matplotlib.colors.LogNorm())\n",
    "im3=axarr[2].imshow(c, norm=matplotlib.colors.LogNorm())\n",
    "axarr[0].set_title('U Plane')\n",
    "axarr[1].set_title('V Plane')\n",
    "axarr[2].set_title('W Plane')\n",
    "bar1=f.colorbar(im,ax=axarr[0],fraction=0.046,pad=0.04)\n",
    "bar2=f.colorbar(im2,ax=axarr[1],fraction=0.046,pad=0.04)\n",
    "bar3=f.colorbar(im3,ax=axarr[2],fraction=0.046,pad=0.04)\n",
    "bar1.set_label('Energy [MeV]')\n",
    "bar2.set_label('Energy [MeV]')\n",
    "bar3.set_label('Energy [MeV]')\n",
    "f.tight_layout() #this is to add more space between plots\n",
    "for ax in axarr.flat:\n",
    "    ax.set(xlabel='Wire Plane', ylabel='Time')\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next cell could maybe help to clean some memory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Run\n",
    "\n",
    "When GPU is used, it runs out of memory durign testing phase..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/500 (0%)]\tLoss: 1.962924\n",
      "Train Epoch: 1 [10/500 (2%)]\tLoss: 0.000573\n",
      "Train Epoch: 1 [20/500 (4%)]\tLoss: 0.000128\n",
      "Train Epoch: 1 [30/500 (6%)]\tLoss: 0.007420\n",
      "Train Epoch: 1 [40/500 (8%)]\tLoss: 2.838897\n",
      "Train Epoch: 1 [50/500 (10%)]\tLoss: 0.001387\n",
      "Train Epoch: 1 [60/500 (12%)]\tLoss: 0.067812\n",
      "Train Epoch: 1 [70/500 (14%)]\tLoss: 0.039685\n",
      "Train Epoch: 1 [80/500 (16%)]\tLoss: 2.582863\n",
      "Train Epoch: 1 [90/500 (18%)]\tLoss: 1.343754\n",
      "Train Epoch: 1 [100/500 (20%)]\tLoss: 4.137868\n",
      "Train Epoch: 1 [110/500 (22%)]\tLoss: 0.014494\n",
      "Train Epoch: 1 [120/500 (24%)]\tLoss: 0.020041\n",
      "Train Epoch: 1 [130/500 (26%)]\tLoss: 0.087289\n",
      "Train Epoch: 1 [140/500 (28%)]\tLoss: 0.681406\n",
      "Train Epoch: 1 [150/500 (30%)]\tLoss: 1.947320\n",
      "Train Epoch: 1 [160/500 (32%)]\tLoss: 0.594068\n",
      "Train Epoch: 1 [170/500 (34%)]\tLoss: 0.009284\n",
      "Train Epoch: 1 [180/500 (36%)]\tLoss: 0.505139\n",
      "Train Epoch: 1 [190/500 (38%)]\tLoss: 2.444731\n",
      "Train Epoch: 1 [200/500 (40%)]\tLoss: 2.800113\n",
      "Train Epoch: 1 [210/500 (42%)]\tLoss: 0.252008\n",
      "Train Epoch: 1 [220/500 (44%)]\tLoss: 0.007999\n",
      "Train Epoch: 1 [230/500 (46%)]\tLoss: 0.048488\n",
      "Train Epoch: 1 [240/500 (48%)]\tLoss: 0.092688\n",
      "Train Epoch: 1 [250/500 (50%)]\tLoss: 0.062656\n",
      "Train Epoch: 1 [260/500 (52%)]\tLoss: 0.399021\n",
      "Train Epoch: 1 [270/500 (54%)]\tLoss: 0.037258\n",
      "Train Epoch: 1 [280/500 (56%)]\tLoss: 0.075144\n",
      "Train Epoch: 1 [290/500 (58%)]\tLoss: 0.010656\n",
      "Train Epoch: 1 [300/500 (60%)]\tLoss: 2.980322\n",
      "Train Epoch: 1 [310/500 (62%)]\tLoss: 0.308115\n",
      "Train Epoch: 1 [320/500 (64%)]\tLoss: 2.089449\n",
      "Train Epoch: 1 [330/500 (66%)]\tLoss: 0.058391\n",
      "Train Epoch: 1 [340/500 (68%)]\tLoss: 0.124389\n",
      "Train Epoch: 1 [350/500 (70%)]\tLoss: 0.005966\n",
      "Train Epoch: 1 [360/500 (72%)]\tLoss: 1.329623\n",
      "Train Epoch: 1 [370/500 (74%)]\tLoss: 0.200486\n",
      "Train Epoch: 1 [380/500 (76%)]\tLoss: 0.021678\n",
      "Train Epoch: 1 [390/500 (78%)]\tLoss: 0.119044\n",
      "Train Epoch: 1 [400/500 (80%)]\tLoss: 0.044870\n",
      "Train Epoch: 1 [410/500 (82%)]\tLoss: 1.257722\n",
      "Train Epoch: 1 [420/500 (84%)]\tLoss: 0.018649\n",
      "Train Epoch: 1 [430/500 (86%)]\tLoss: 2.059562\n",
      "Train Epoch: 1 [440/500 (88%)]\tLoss: 0.088676\n",
      "Train Epoch: 1 [450/500 (90%)]\tLoss: 3.006977\n",
      "Train Epoch: 1 [460/500 (92%)]\tLoss: 2.228224\n",
      "Train Epoch: 1 [470/500 (94%)]\tLoss: 2.012183\n",
      "Train Epoch: 1 [480/500 (96%)]\tLoss: 0.081531\n",
      "Train Epoch: 1 [490/500 (98%)]\tLoss: 0.112455\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.19 GiB (GPU 0; 5.93 GiB total capacity; 1.77 GiB already allocated; 792.50 MiB free; 1.79 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-dd6882d7af24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-1dd845ac2e4d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a724a22c6301>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(args, model, device, test_loader)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# sum up batch loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# get the index of the max log-probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a724a22c6301>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         return F.log_softmax(\n\u001b[0;32m---> 23\u001b[0;31m             super(MnistResNet, self).forward(x), dim=-1)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# See note [TorchScript super()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1668\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1669\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1670\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1671\u001b[0m     )\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.19 GiB (GPU 0; 5.93 GiB total capacity; 1.77 GiB already allocated; 792.50 MiB free; 1.79 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "start.record()\n",
    "main()\n",
    "end.record()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(start.elapsed_time(end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non resnet:\n",
    "# with gpu is was 129028.3828125 ms, that is ~130 s for 14 epochs - final test set \n",
    "# Test set: Average loss: 0.0270, Accuracy: 9915/10000 (99%)\n",
    "# with cpu is was 1085891.5 ms, that is ~1085 s ~ 18 m for 14 epochs - final test set loss 0.0295 \n",
    "#accuracy 9911/10000 (99%)\n",
    "\n",
    "#resnet50\n",
    "# with gpu is was 1124874.75 ms, that is ~1125 s ~ 19 m for 14 epochs - final test set batch size 64\n",
    "# Test set: Average loss: -0.8874, Accuracy: 8875/10000 (89%)\n",
    "#resnet50\n",
    "# with gpu is was 978600.0625 ms, that is ~978 s ~ 16 m for 14 epochs - final test set batch size 128\n",
    "# Test set: Average loss: -0.9861, Accuracy: 9860/10000 (99%)\n",
    "#resnet50 size 32x32\n",
    "# with gpu is was 1012531.75 ms, that is ~1012 s ~ 17 m for 14 epochs - final test set batch size 128\n",
    "# Test set: Average loss: -0.8869, Accuracy: 8869/10000 (89%)\n",
    "\n",
    "#resnet50 size 32x32 log_softmax (dim=1) and nll_loss (10-02-2020)\n",
    "# with gpu is was 996138.8125 ms, that is ~996 s ~ 17 m for 14 epochs - final test set batch size 128\n",
    "# Test set: Average loss: 0.0236, Accuracy: 9942/10000 (99%)\n",
    "\n",
    "#resnet50 size 32x32 log_softmax (dim=-1) and nll_loss (10-02-2020)\n",
    "# with gpu is was 992216.3125 ms, that is ~992 s ~ 16 m for 14 epochs - final test set batch size 128\n",
    "# Test set: Average loss: 0.0180, Accuracy: 9950/10000 (100%)\n",
    "\n",
    "#resnet50 size 32x32 log_softmax (dim=-1) and nll_loss (10-02-2020)\n",
    "# with gpu is was 427369.625 ms, that is ~427 s ~ 7 m for 6 epochs - final test set batch size 128\n",
    "# Test set: Average loss: 0.0217, Accuracy: 9938/10000 (99%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_array = []\n",
    "for i in range (1,epochs+1):\n",
    "    epochs_array.append(i)\n",
    "    \n",
    "accuracy=np.array(accuracy)\n",
    "accuracy=accuracy/100\n",
    "epochs_array = np.array(epochs_array)\n",
    "loss = np.array(loss)\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Accuracy vs. Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.plot(epochs_array,accuracy)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Total Loss vs. Epoch')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.plot(epochs_array,loss,color='C1')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_array_total = [] # array containing all predictions for all epochs\n",
    "pred_array = [] #array containing only predictions of last epoch\n",
    "for i in range(len(pred_list)):\n",
    "    pred_array_total.append(pred_list[i].cpu().numpy())\n",
    "    \n",
    "pred_array_total=np.array(pred_array_total)\n",
    "pred_array_total=np.squeeze(np.concatenate(pred_array_total))\n",
    "#print(epochs*test_batch)\n",
    "for i in range ((epochs*(test_batch*500)-500),(epochs*test_batch*500)):\n",
    "    pred_array.append(pred_array_total[i])\n",
    "    \n",
    "pred_array=np.array(pred_array)\n",
    "pred_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_array_total = []#array containing all the targets for all epochs\n",
    "target_array = []#array containing targets for last epoch\n",
    "for i in range(len(target_list)):\n",
    "    target_array_total.append(target_list[i].cpu().numpy())\n",
    "    \n",
    "target_array_total=np.array(target_array_total)\n",
    "target_array_total=np.squeeze(np.concatenate(target_array_total))\n",
    "\n",
    "for i in range ((epochs*test_batch*500)-500,epochs*test_batch*500):\n",
    "    target_array.append(target_array_total[i])\n",
    "    \n",
    "target_array=np.array(target_array)\n",
    "target_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "wrong_list = []\n",
    "for i in range(len(target_array)):\n",
    "    if target_array[i]!=pred_array[i]:\n",
    "        wrong_list.append(i)\n",
    "        \n",
    "element=random.choice(wrong_list)\n",
    "print('This was supposed to be ',target_array[element],' but it was predicted as ',pred_array[element])\n",
    "x, _ = dataset_test[element] # x is now a torch.Tensor\n",
    "plt.imshow(x.numpy()[0], cmap='gray')\n",
    "\n",
    "a = x_train_cpu[(starting_number),:,:,0]\n",
    "\n",
    "b = x_train_cpu[(starting_number),:,:,1]\n",
    "\n",
    "c = x_train_cpu[(starting_number),:,:,2]\n",
    "\n",
    "\n",
    "f, axarr = plt.subplots(1,3,figsize=figsize)#, constrained_layout=True)\n",
    "f.suptitle('Event Seen From Wire Planes - 200 x 200 pixels - this is a '+truth_stg+' event')\n",
    "im=axarr[0].imshow(a, norm=matplotlib.colors.LogNorm())\n",
    "im2=axarr[1].imshow(b, norm=matplotlib.colors.LogNorm())\n",
    "im3=axarr[2].imshow(c, norm=matplotlib.colors.LogNorm())\n",
    "axarr[0].set_title('U Plane')\n",
    "axarr[1].set_title('V Plane')\n",
    "axarr[2].set_title('W Plane')\n",
    "bar1=f.colorbar(im,ax=axarr[0],fraction=0.046,pad=0.04)\n",
    "bar2=f.colorbar(im2,ax=axarr[1],fraction=0.046,pad=0.04)\n",
    "bar3=f.colorbar(im3,ax=axarr[2],fraction=0.046,pad=0.04)\n",
    "bar1.set_label('Energy [MeV]')\n",
    "bar2.set_label('Energy [MeV]')\n",
    "bar3.set_label('Energy [MeV]')\n",
    "f.tight_layout() #this is to add more space between plots\n",
    "for ax in axarr.flat:\n",
    "    ax.set(xlabel='Wire Plane', ylabel='Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "torch.set_printoptions(linewidth = 120)\n",
    "import sklearn\n",
    "#print(sklearn.__version__)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "\n",
    "plt.figure(figsize=(50,50))\n",
    "cm=sklearn.metrics.confusion_matrix(target_array, pred_array,normalize='all')#, classes=label_dict,\n",
    "                     #title='Confusion matrix')\n",
    "cm\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "df_cm = pd.DataFrame(cm, index = [i for i in \"01\"],\n",
    "                  columns = [i for i in \"01\"])\n",
    "plt.figure(figsize = (14,10))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
