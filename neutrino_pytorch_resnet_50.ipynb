{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet-50 with Neutrino Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing essential libraries\n",
    "this will be cleaned, some are inessential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from torchvision.models.resnet import ResNet, BasicBlock, Bottleneck\n",
    "from torchvision.datasets import MNIST\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import inspect\n",
    "import time\n",
    "from torch import nn, optim\n",
    "import torch\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I check here if cuda gpu is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, Train, and Test\n",
    "\n",
    "I define here the model (ResNet-50) and the train and test function. The model takes as input images channel first with 3 channels. The test definition is week and it could lead to a memory leak. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred_list and target_list are two lists of cuda tensors containing respectively the predictions and the \n",
    "#target values. Each tensor will be test_batch_size in lenght. Each list will contain \n",
    "#(amount of test samples/test_batch_size)*epochs samples. E.g. with 10k test samples, test_batch_size = 1k and\n",
    "#4 epochs you'll get a list of 40 elements, each element 1k. Further down these lists will be converted in arrays.\n",
    "\n",
    "pred_list = []\n",
    "target_list = []\n",
    "accuracy = []\n",
    "loss = []\n",
    "accuracy_train = []\n",
    "loss_train = []\n",
    "epochs = 10\n",
    "test_batch = 100\n",
    "    \n",
    "class MnistResNet(ResNet):\n",
    "    def __init__(self):\n",
    "        super(MnistResNet, self).__init__(Bottleneck, [3,4,6,3], num_classes=3)\n",
    "        self.conv1 = torch.nn.Conv2d(3, 64,\n",
    "            kernel_size=(7, 7), \n",
    "            stride=(2, 2), \n",
    "            padding=(3, 3), bias=False)\n",
    "######if you then use nll_loss you have to use log_sotmax        \n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(\n",
    "            super(MnistResNet, self).forward(x), dim=-1)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    train_loss = 0   \n",
    "    correct = 0\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        output = None\n",
    "        del output\n",
    "        target = None\n",
    "        del target\n",
    "        length = len(data)\n",
    "        data = None\n",
    "        del data\n",
    "        pred = None\n",
    "        del pred\n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * length, len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    print('\\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        train_loss, correct, len(train_loader.dataset),\n",
    "        100. * correct / len(train_loader.dataset)))\n",
    "    accuracy_train.append(100. * correct / len(train_loader.dataset))\n",
    "    loss_train.append(train_loss)\n",
    "\n",
    "\n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            data = None\n",
    "            del data\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            pred_list.append(pred)\n",
    "            target_list.append(target)\n",
    "            output = None\n",
    "            del output\n",
    "            pred = None\n",
    "            del pred\n",
    "            \n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    accuracy.append(100. * correct / len(test_loader.dataset))\n",
    "    loss.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "I implemented here a simple dataset. At the moment, it takes a tensor as data and numpy array as target. Moreover, it only takes one file. This is something that I will need to change in the future since we have several files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing DataSet\n",
    "\n",
    "class NeutrinoDataset(Dataset):\n",
    "    \"\"\"The training table dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        #x_filenames = glob(x_path + '*.png') # Get the filenames of all training images\n",
    "        \n",
    "        #self.x_data = [torch.from_numpy(misc.imread(filename)) for filename in x_filenames] # Load the images into torch tensors\n",
    "        #self.y_data = target_label_list # Class labels\n",
    "        self.x_data = x_tensor\n",
    "        self.y_data = y_tensor\n",
    "        self.len = len(self.x_data)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_temp_0 = np.load('/home/stefano/neutrino/data/final_array_x/train/x_train_numu_500_0.npy')\n",
    "y_train_0 = np.load('/home/stefano/neutrino/data/final_array_y/train/y_train_numu_500_event_0.npy')\n",
    "\n",
    "x_train_temp_1 = np.load('/home/stefano/neutrino/data/final_array_x/train/x_train_numu_500_1.npy')\n",
    "y_train_1 = np.load('/home/stefano/neutrino/data/final_array_y/train/y_train_numu_500_event_1.npy')\n",
    "\n",
    "x_train_temp_2 = np.load('/home/stefano/neutrino/data/final_array_x/train/x_train_numu_500_2.npy')\n",
    "y_train_2 = np.load('/home/stefano/neutrino/data/final_array_y/train/y_train_numu_500_event_2.npy')\n",
    "\n",
    "x_train_numu_3 = np.load('/home/stefano/neutrino/data/final_array_x/train/x_train_numu_500_3.npy')\n",
    "y_train_numu_3 = np.load('/home/stefano/neutrino/data/final_array_y/train/y_train_numu_500_event_3.npy')\n",
    "\n",
    "x_train_numu_4 = np.load('/home/stefano/neutrino/data/final_array_x/train/x_train_numu_500_4.npy')\n",
    "y_train_numu_4 = np.load('/home/stefano/neutrino/data/final_array_y/train/y_train_numu_500_event_4.npy')\n",
    "\n",
    "x_train_numu_5 = np.load('/home/stefano/neutrino/data/final_array_x/train/x_train_numu_500_5.npy')\n",
    "y_train_numu_5 = np.load('/home/stefano/neutrino/data/final_array_y/train/y_train_numu_500_event_5.npy')\n",
    "\n",
    "x_train_numu_6 = np.load('/home/stefano/neutrino/data/final_array_x/train/x_train_numu_500_6.npy')\n",
    "y_train_numu_6 = np.load('/home/stefano/neutrino/data/final_array_y/train/y_train_numu_500_event_6.npy')\n",
    "\n",
    "x_train_numu_7 = np.load('/home/stefano/neutrino/data/final_array_x/train/x_train_numu_500_7.npy')\n",
    "y_train_numu_7 = np.load('/home/stefano/neutrino/data/final_array_y/train/y_train_numu_500_event_7.npy')\n",
    "\n",
    "x_train_numu_8 = np.load('/home/stefano/neutrino/data/final_array_x/train/x_train_numu_500_8.npy')\n",
    "y_train_numu_8 = np.load('/home/stefano/neutrino/data/final_array_y/train/y_train_numu_500_event_8.npy')\n",
    "\n",
    "x_train_numu_9 = np.load('/home/stefano/neutrino/data/final_array_x/train/x_train_numu_500_9.npy')\n",
    "y_train_numu_9 = np.load('/home/stefano/neutrino/data/final_array_y/train/y_train_numu_500_event_9.npy')\n",
    "\n",
    "x_train_temp_3 = np.load('/home/stefano/neutrino/data/final_array_x/train/x_train_nue_500_0.npy')\n",
    "y_train_3 = np.load('/home/stefano/neutrino/data/final_array_y/train/y_train_nue_500_event_0.npy')\n",
    "\n",
    "x_train_temp_4 = np.load('/home/stefano/neutrino/data/final_array_x/train/x_train_nue_500_1.npy')\n",
    "y_train_4 = np.load('/home/stefano/neutrino/data/final_array_y/train/y_train_nue_500_event_1.npy')\n",
    "\n",
    "x_train_temp_5 = np.load('/home/stefano/neutrino/data/final_array_x/train/x_train_nue_500_2.npy')\n",
    "y_train_5 = np.load('/home/stefano/neutrino/data/final_array_y/train/y_train_nue_500_event_2.npy')\n",
    "\n",
    "x_train_temp_6 = np.load('/home/stefano/neutrino/data/final_array_x/train/x_train_nue_500_3.npy')\n",
    "y_train_6 = np.load('/home/stefano/neutrino/data/final_array_y/train/y_train_nue_500_event_3.npy')\n",
    "\n",
    "x_train_nue_4 = np.load('/home/stefano/neutrino/data/final_array_x/train/x_train_nue_500_4.npy')\n",
    "y_train_nue_4 = np.load('/home/stefano/neutrino/data/final_array_y/train/y_train_nue_500_event_4.npy')\n",
    "\n",
    "x_train_nue_5 = np.load('/home/stefano/neutrino/data/final_array_x/train/x_train_nue_500_5.npy')\n",
    "y_train_nue_5 = np.load('/home/stefano/neutrino/data/final_array_y/train/y_train_nue_500_event_5.npy')\n",
    "\n",
    "x_train_nue_6 = np.load('/home/stefano/neutrino/data/final_array_x/train/x_train_nue_500_6.npy')\n",
    "y_train_nue_6 = np.load('/home/stefano/neutrino/data/final_array_y/train/y_train_nue_500_event_6.npy')\n",
    "\n",
    "x_train_nue_7 = np.load('/home/stefano/neutrino/data/final_array_x/train/x_train_nue_500_7.npy')\n",
    "y_train_nue_7 = np.load('/home/stefano/neutrino/data/final_array_y/train/y_train_nue_500_event_7.npy')\n",
    "\n",
    "x_train_nue_8 = np.load('/home/stefano/neutrino/data/final_array_x/train/x_train_nue_500_8.npy')\n",
    "y_train_nue_8 = np.load('/home/stefano/neutrino/data/final_array_y/train/y_train_nue_500_event_8.npy')\n",
    "\n",
    "x_train_nue_9 = np.load('/home/stefano/neutrino/data/final_array_x/train/x_train_nue_500_9.npy')\n",
    "y_train_nue_9 = np.load('/home/stefano/neutrino/data/final_array_y/train/y_train_nue_500_event_9.npy')\n",
    "                         \n",
    "x_test_numu_0 = np.load('/home/stefano/neutrino/data/final_array_x/test/x_test_numu_500_0.npy')\n",
    "y_test_numu_0 = np.load('/home/stefano/neutrino/data/final_array_y/test/y_test_numu_500_event_0.npy')\n",
    "\n",
    "x_test_numu_1 = np.load('/home/stefano/neutrino/data/final_array_x/test/x_test_numu_500_1.npy')\n",
    "y_test_numu_1 = np.load('/home/stefano/neutrino/data/final_array_y/test/y_test_numu_500_event_1.npy')\n",
    "\n",
    "x_test_nue_0 = np.load('/home/stefano/neutrino/data/final_array_x/test/x_test_nue_500_0.npy')\n",
    "y_test_nue_0 = np.load('/home/stefano/neutrino/data/final_array_y/test/y_test_nue_500_event_0.npy')\n",
    "\n",
    "x_test_nue_1 = np.load('/home/stefano/neutrino/data/final_array_x/test/x_test_nue_500_1.npy')\n",
    "y_test_nue_1 = np.load('/home/stefano/neutrino/data/final_array_y/test/y_test_nue_500_event_1.npy')\n",
    "    \n",
    "\n",
    "#concatenate all the files together and then delete from the memory unnecessary files\n",
    "x_train_temp_a=np.concatenate((x_train_temp_0, x_train_temp_1))\n",
    "x_train_temp_b=np.concatenate((x_train_temp_a, x_train_temp_2))\n",
    "x_train_temp_0 = None\n",
    "del x_train_temp_0\n",
    "x_train_temp_1 = None\n",
    "del x_train_temp_1\n",
    "x_train_temp_2 = None\n",
    "del x_train_temp_2\n",
    "x_train_temp_a = None\n",
    "del x_train_temp_a\n",
    "x_train_temp_c=np.concatenate((x_train_temp_b, x_train_temp_3))\n",
    "x_train_temp_b=None\n",
    "del x_train_temp_b\n",
    "x_train_temp_d=np.concatenate((x_train_temp_c, x_train_temp_4))\n",
    "x_train_temp_c=None\n",
    "del x_train_temp_c\n",
    "x_train_temp_e=np.concatenate((x_train_temp_d, x_train_temp_5))\n",
    "x_train_temp_3 = None\n",
    "del x_train_temp_3\n",
    "x_train_temp_4 = None\n",
    "del x_train_temp_4\n",
    "x_train_temp_5 = None\n",
    "del x_train_temp_5\n",
    "x_train_temp_d = None\n",
    "del x_train_temp_d\n",
    "x_train_temp_f=np.concatenate((x_train_temp_e, x_train_temp_6))\n",
    "x_train_temp_e=None\n",
    "del x_train_temp_e\n",
    "x_train_temp_6 = None\n",
    "del x_train_temp_6\n",
    "x_train_temp_g=np.concatenate((x_train_temp_f, x_train_numu_3))\n",
    "x_train_temp_f=None\n",
    "del x_train_temp_f\n",
    "x_train_numu_3 = None\n",
    "del x_train_numu_3\n",
    "x_train_temp_h=np.concatenate((x_train_temp_g, x_train_nue_4))\n",
    "x_train_temp_g=None\n",
    "del x_train_temp_g\n",
    "x_train_nue_4 = None\n",
    "del x_train_nue_4\n",
    "x_train_temp_i=np.concatenate((x_train_temp_h, x_train_numu_4))\n",
    "x_train_temp_h=None\n",
    "del x_train_temp_h\n",
    "x_train_numu_4 = None\n",
    "del x_train_numu_4\n",
    "x_train_temp_l=np.concatenate((x_train_temp_i, x_train_nue_5))\n",
    "x_train_temp_i=None\n",
    "del x_train_temp_i\n",
    "x_train_nue_5 = None\n",
    "del x_train_nue_5\n",
    "x_train_temp_m=np.concatenate((x_train_temp_l, x_train_numu_5))\n",
    "x_train_temp_l=None\n",
    "del x_train_temp_l\n",
    "x_train_numu_5 = None\n",
    "del x_train_numu_5\n",
    "x_train_temp_n=np.concatenate((x_train_temp_m, x_train_nue_6))\n",
    "x_train_temp_m=None\n",
    "del x_train_temp_m\n",
    "x_train_nue_6 = None\n",
    "del x_train_nue_6\n",
    "x_train_temp_o=np.concatenate((x_train_temp_n, x_train_numu_6))\n",
    "x_train_temp_n=None\n",
    "del x_train_temp_n\n",
    "x_train_numu_6 = None\n",
    "del x_train_numu_6\n",
    "x_train_temp_p=np.concatenate((x_train_temp_o, x_train_nue_7))\n",
    "x_train_temp_o=None\n",
    "del x_train_temp_o\n",
    "x_train_nue_7 = None\n",
    "del x_train_nue_7\n",
    "x_train_temp_q=np.concatenate((x_train_temp_p, x_train_numu_7))\n",
    "x_train_temp_p=None\n",
    "del x_train_temp_p\n",
    "x_train_numu_7 = None\n",
    "del x_train_numu_7\n",
    "x_train_temp_r=np.concatenate((x_train_temp_q, x_train_nue_8))\n",
    "x_train_temp_q=None\n",
    "del x_train_temp_q\n",
    "x_train_nue_8 = None\n",
    "del x_train_nue_8\n",
    "x_train_temp_s=np.concatenate((x_train_temp_r, x_train_numu_8))\n",
    "x_train_temp_r=None\n",
    "del x_train_temp_r\n",
    "x_train_numu_8 = None\n",
    "del x_train_numu_8\n",
    "x_train_temp_t=np.concatenate((x_train_temp_s, x_train_nue_9))\n",
    "x_train_temp_s=None\n",
    "del x_train_temp_s\n",
    "x_train_nue_9 = None\n",
    "del x_train_nue_9\n",
    "x_train_temp=np.concatenate((x_train_temp_t, x_train_numu_9))\n",
    "x_train_temp_t=None\n",
    "del x_train_temp_t\n",
    "x_train_numu_9 = None\n",
    "del x_train_numu_9\n",
    "\n",
    "\n",
    "\n",
    "x_test_temp_a=np.concatenate((x_test_numu_0, x_test_nue_0))\n",
    "x_test_numu_0 = None\n",
    "del x_test_numu_0\n",
    "x_test_nue_0 = None\n",
    "del x_test_nue_0\n",
    "x_test_temp_b=np.concatenate((x_test_temp_a, x_test_nue_1))\n",
    "x_test_temp_a = None\n",
    "del x_test_temp_a\n",
    "x_test_nue_1 = None\n",
    "del x_test_nue_1\n",
    "x_test_temp=np.concatenate((x_test_temp_b, x_test_numu_1))\n",
    "x_test_temp_b = None\n",
    "del x_test_temp_b\n",
    "x_test_numu_1 = None\n",
    "del x_test_numu_1\n",
    "\n",
    "y_train_a=np.concatenate((y_train_0, y_train_1))\n",
    "y_train_0 = None\n",
    "del y_train_0\n",
    "y_train_1 = None\n",
    "del y_train_1\n",
    "y_train_b=np.concatenate((y_train_a, y_train_2))\n",
    "y_train_c=np.concatenate((y_train_b, y_train_3))\n",
    "y_train_a=None\n",
    "del y_train_a\n",
    "y_train_b=None\n",
    "del y_train_b\n",
    "y_train_d=np.concatenate((y_train_c, y_train_4))\n",
    "y_train_c=None\n",
    "del y_train_c\n",
    "y_train_e=np.concatenate((y_train_d, y_train_5))\n",
    "y_train_2 = None\n",
    "del y_train_2\n",
    "y_train_3 = None\n",
    "del y_train_3\n",
    "y_train_4 = None\n",
    "del y_train_4\n",
    "y_train_5 = None\n",
    "del y_train_5\n",
    "y_train_d=None\n",
    "del y_train_d\n",
    "y_train_f=np.concatenate((y_train_e, y_train_6))\n",
    "y_train_e=None\n",
    "del y_train_e\n",
    "y_train_6 = None\n",
    "del y_train_6\n",
    "y_train_g=np.concatenate((y_train_f, y_train_numu_3))\n",
    "y_train_f=None\n",
    "del y_train_f\n",
    "y_train_numu_3 = None\n",
    "del y_train_numu_3\n",
    "y_train_h=np.concatenate((y_train_g, y_train_nue_4))\n",
    "y_train_g=None\n",
    "del y_train_g\n",
    "y_train_nue_4 = None\n",
    "del y_train_nue_4\n",
    "y_train_i=np.concatenate((y_train_h, y_train_numu_4))\n",
    "y_train_h=None\n",
    "del y_train_h\n",
    "y_train_numu_4 = None\n",
    "del y_train_numu_4\n",
    "y_train_l=np.concatenate((y_train_i, y_train_nue_5))\n",
    "y_train_i=None\n",
    "del y_train_i\n",
    "y_train_nue_5 = None\n",
    "del y_train_nue_5\n",
    "y_train_m=np.concatenate((y_train_l, y_train_numu_5))\n",
    "y_train_l=None\n",
    "del y_train_l\n",
    "y_train_numu_5 = None\n",
    "del y_train_numu_5\n",
    "y_train_n=np.concatenate((y_train_m, y_train_nue_6))\n",
    "y_train_m=None\n",
    "del y_train_m\n",
    "y_train_nue_6 = None\n",
    "del y_train_nue_6\n",
    "y_train_o=np.concatenate((y_train_n, y_train_numu_6))\n",
    "y_train_n=None\n",
    "del y_train_n\n",
    "y_train_numu_6 = None\n",
    "del y_train_numu_6\n",
    "y_train_p=np.concatenate((y_train_o, y_train_nue_7))\n",
    "y_train_o=None\n",
    "del y_train_o\n",
    "y_train_nue_7 = None\n",
    "del y_train_nue_7\n",
    "y_train_q=np.concatenate((y_train_p, y_train_numu_7))\n",
    "y_train_p=None\n",
    "del y_train_p\n",
    "y_train_numu_7 = None\n",
    "del y_train_numu_7\n",
    "y_train_r=np.concatenate((y_train_q, y_train_nue_8))\n",
    "y_train_q=None\n",
    "del y_train_q\n",
    "y_train_nue_8 = None\n",
    "del y_train_nue_8\n",
    "y_train_s=np.concatenate((y_train_r, y_train_numu_8))\n",
    "y_train_r=None\n",
    "del y_train_r\n",
    "y_train_numu_8 = None\n",
    "del y_train_numu_8\n",
    "y_train_t=np.concatenate((y_train_s, y_train_nue_9))\n",
    "y_train_s=None\n",
    "del y_train_s\n",
    "y_train_nue_9 = None\n",
    "del y_train_nue_9\n",
    "y_train=np.concatenate((y_train_t, y_train_numu_9))\n",
    "y_train_t=None\n",
    "del y_train_t\n",
    "y_train_numu_9 = None\n",
    "del y_train_numu_9\n",
    "\n",
    "\n",
    "y_test_a=np.concatenate((y_test_numu_0, y_test_nue_0))\n",
    "y_test_numu_0 = None\n",
    "del y_test_numu_0\n",
    "y_test_nue_0 = None\n",
    "del y_test_nue_0\n",
    "y_test_b=np.concatenate((y_test_a, y_test_nue_1))\n",
    "y_test_a = None\n",
    "del y_test_a\n",
    "y_test_nue_1 = None\n",
    "del y_test_nue_1\n",
    "y_test=np.concatenate((y_test_b, y_test_numu_1))\n",
    "y_test_b = None\n",
    "del y_test_b\n",
    "y_test_numu_1 = None\n",
    "del y_test_numu_1\n",
    "\n",
    "print('x_train_temp.shape',x_train_temp.shape)\n",
    "print('x_test_temp.shape',x_test_temp.shape)\n",
    "print('y_train.shape',y_train.shape)\n",
    "print('y_test.shape',y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing files and preparing the dataset\n",
    "\n",
    "In this part I import the train and test files, I normalize them and I reshape them to be channel first. This part could also lead to potential memory leaks. It would be nice to convert and normalise the numpy array in one go, but at the moment I can't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###this method is broken!!! do not use until fixed\n",
    "\n",
    "import os\n",
    "\n",
    "folder_x_train='/home/stefano/neutrino/data/final_array_x/train/'\n",
    "directory_x_train=os.fsencode(folder_x_train)\n",
    "\n",
    "folder_x_test='/home/stefano/neutrino/data/final_array_x/test/'\n",
    "directory_x_test=os.fsencode(folder_x_test)\n",
    "\n",
    "folder_y_train='/home/stefano/neutrino/data/final_array_y/train/'\n",
    "directory_y_train=os.fsencode(folder_y_train)\n",
    "\n",
    "folder_y_test='/home/stefano/neutrino/data/final_array_y/test/'\n",
    "directory_y_test=os.fsencode(folder_y_test)\n",
    "\n",
    "counter_a=0\n",
    "counter_b=0\n",
    "counter_c=0\n",
    "counter_d=0\n",
    "\n",
    "for file in os.listdir(directory_x_train):\n",
    "    x_train=np.load(folder_x_train+os.fsdecode(file))\n",
    "    if counter_a==0:\n",
    "        x_train_temp=x_train\n",
    "    elif counter_a>0:\n",
    "        x_train_temp=np.concatenate((x_train_temp,x_train))\n",
    "        \n",
    "    counter_a=counter_a+1\n",
    "    del x_train\n",
    "        \n",
    "for file in os.listdir(directory_x_test):\n",
    "    x_test=np.load(folder_x_test+os.fsdecode(file))\n",
    "    if counter_b==0:\n",
    "        x_test_temp=x_test\n",
    "    elif counter_b>0:\n",
    "        x_test_temp=np.concatenate((x_test_temp,x_test))\n",
    "        \n",
    "    counter_b=counter_b+1\n",
    "    del x_test\n",
    "\n",
    "for file in os.listdir(directory_y_train):\n",
    "    y_tr=np.load(folder_y_train+os.fsdecode(file))\n",
    "    if counter_c==0:\n",
    "        y_train=y_tr\n",
    "    elif counter_c>0:\n",
    "        y_train=np.concatenate((y_train,y_tr))\n",
    "        \n",
    "    counter_c=counter_c+1\n",
    "    del y_tr\n",
    "    \n",
    "for file in os.listdir(directory_y_test):\n",
    "    y_ts=np.load(folder_y_test+os.fsdecode(file))\n",
    "    if counter_d==0:\n",
    "        y_test=y_ts\n",
    "    elif counter_d>0:\n",
    "        y_test=np.concatenate((y_test,y_ts))\n",
    "        \n",
    "    counter_d=counter_d+1\n",
    "    del y_ts\n",
    "    \n",
    "print(x_train_temp.shape)\n",
    "print(x_test_temp.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "del counter_a\n",
    "del counter_b\n",
    "del counter_c\n",
    "del counter_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#passages are: x_train_temp -> x_train (min 0 max 1, float32) -> x_train_reshaped (channel first) -> x_train_tensor\n",
    "\n",
    "#normalise the data\n",
    "old_min = np.min(x_train_temp)\n",
    "old_max = np.max(x_train_temp)\n",
    "print(\"Before:\", old_min, old_max)\n",
    "x_train = x_train_temp.astype('float32')\n",
    "x_test = x_test_temp.astype('float32')\n",
    "x_train /=255\n",
    "x_test /= 255\n",
    "new_min = np.min(x_train)\n",
    "new_max = np.max(x_train)\n",
    "print(\"After:\", new_min, new_max)\n",
    "\n",
    "print(\"Test Data Shape after conversion to float\")\n",
    "    \n",
    "    #read the dimensions from one example in the trainig set\n",
    "img_rows, img_cols = x_train[0].shape[0], x_train[0].shape[1]\n",
    "print(x_train[0].shape[0])\n",
    "print(x_train[0].shape[1])\n",
    "\n",
    "#read the dimensions from one example in the trainig set\n",
    "img_rows, img_cols = x_train[0].shape[0], x_train[0].shape[1]\n",
    "\n",
    "#Different NN libraries (e.g., TF) use different ordering of dimensions\n",
    "#Here we set the \"input shape\" so that later the NN knows what shape to expect\n",
    "x_train_reshaped = x_train.reshape(x_train.shape[0], 3, img_rows, img_cols)\n",
    "x_test_reshaped = x_test.reshape(x_test.shape[0], 3, img_rows, img_cols)\n",
    "input_shape = (3, img_rows, img_cols)\n",
    "\n",
    "print(\"input_shape\",input_shape)\n",
    "print(\"x_train[0].shape[0]\", x_train_reshaped[0].shape[0])\n",
    "print(\"x_train[0].shape[1]\", x_train_reshaped[0].shape[1])\n",
    "print(\"x_train[0].shape[2]\", x_train_reshaped[0].shape[2])\n",
    "print(\"x_train.shape\",x_train.shape)\n",
    "\n",
    "x_train_tensor = torch.from_numpy(x_train_reshaped).float()\n",
    "x_test_tensor = torch.from_numpy(x_test_reshaped).float()\n",
    "\n",
    "x_train = None\n",
    "del x_train\n",
    "x_test = None\n",
    "del x_test\n",
    "x_train_reshaped = None\n",
    "del x_train_reshaped\n",
    "x_test_reshaped = None\n",
    "del x_test_reshaped\n",
    "\n",
    "y_train_tensor=torch.from_numpy(y_train)\n",
    "y_test_tensor=torch.from_numpy(y_test)\n",
    "\n",
    "#y_train = None\n",
    "#del y_train\n",
    "#y_test = None\n",
    "#del y_test\n",
    "\n",
    "dataset_test = NeutrinoDataset(x_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of main function\n",
    "\n",
    "I define here the dataset and the main function. This seems to be the stronger part of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easydict\n",
    "def main():\n",
    "    args = easydict.EasyDict({\n",
    "        \"batch_size\": 16,#it was 64\n",
    "        \"test_batch_size\": test_batch,\n",
    "        \"epochs\": epochs,\n",
    "        \"lr\": 1.0,\n",
    "        \"gamma\": 0.7,\n",
    "        \"no_cuda\": False,\n",
    "        \"seed\": 1,\n",
    "        \"log_interval\":10,\n",
    "        \"save_model\": False\n",
    "    })\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    #device = torch.device(\"cpu\")\n",
    "    \n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    \n",
    "    dataset_train = NeutrinoDataset(x_train_tensor, y_train_tensor)\n",
    "    dataset_test = NeutrinoDataset(x_test_tensor, y_test_tensor)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(dataset=dataset_train,\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset_test,\n",
    "        batch_size=args.test_batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    model = MnistResNet().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(args, model, device, test_loader)\n",
    "        scheduler.step()\n",
    "\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), \"mnist_cnn.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting your Train and Test Dataset\n",
    "It is important to always check your dataset. With the following lines images from your train and test dataset and randomly selected and plotted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part I calcolate mean and std of the pics in case I find a way to use tensor.normalise. Skip this part when run the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "\n",
    "The following two cells contain a cool tool to visualise train and test dataset. Useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda =  torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "import zlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "figsize = [14, 5]\n",
    "starting_number = torch.IntTensor.item(torch.LongTensor(1).random_(0, len(x_train_temp)-1))#-(nrows*ncols)))\n",
    "\n",
    "print(x_train_temp.shape)\n",
    "#if use_cuda == True:\n",
    "#    x_train_cpu=x_train_temp.cpu()\n",
    "#    y_train_cpu=y_train_tensor.cpu()\n",
    "#else:\n",
    "#    x_train_cpu=x_train_temp\n",
    "#    y_train_cpu=y_train_tensor\n",
    "    \n",
    "print(\"now showing image \",(starting_number))\n",
    "\n",
    "####this part is for the true value\n",
    "#0 is for NC events\n",
    "#1 is for numu CC events\n",
    "#2 is for nue CC events\n",
    "truth=y_train[starting_number]\n",
    "if truth==0:\n",
    "    truth_stg=\"NC\"\n",
    "elif truth==1:\n",
    "    truth_stg=\"numu CC\"\n",
    "elif truth==2:\n",
    "    truth_stg=\"nue CC\"\n",
    "\n",
    "a = x_train_temp[(starting_number),:,:,0]\n",
    "\n",
    "b = x_train_temp[(starting_number),:,:,1]\n",
    "\n",
    "c = x_train_temp[(starting_number),:,:,2]\n",
    "\n",
    "\n",
    "f, axarr = plt.subplots(1,3,figsize=figsize)#, constrained_layout=True)\n",
    "f.suptitle('Event Seen From Wire Planes - 200 x 200 pixels - this is a '+truth_stg+' event')\n",
    "im=axarr[0].imshow(a, norm=matplotlib.colors.LogNorm())\n",
    "im2=axarr[1].imshow(b, norm=matplotlib.colors.LogNorm())\n",
    "im3=axarr[2].imshow(c, norm=matplotlib.colors.LogNorm())\n",
    "axarr[0].set_title('U Plane')\n",
    "axarr[1].set_title('V Plane')\n",
    "axarr[2].set_title('W Plane')\n",
    "bar1=f.colorbar(im,ax=axarr[0],fraction=0.046,pad=0.04)\n",
    "bar2=f.colorbar(im2,ax=axarr[1],fraction=0.046,pad=0.04)\n",
    "bar3=f.colorbar(im3,ax=axarr[2],fraction=0.046,pad=0.04)\n",
    "bar1.set_label('Energy [MeV]')\n",
    "bar2.set_label('Energy [MeV]')\n",
    "bar3.set_label('Energy [MeV]')\n",
    "f.tight_layout() #this is to add more space between plots\n",
    "for ax in axarr.flat:\n",
    "    ax.set(xlabel='Wire Plane', ylabel='Time')\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figsize = [14, 5]\n",
    "starting_number = torch.IntTensor.item(torch.LongTensor(1).random_(0, len(x_test_temp)-1))#-(nrows*ncols)))\n",
    "    \n",
    "print(\"now showing image \",(starting_number))\n",
    "\n",
    "####this part is for the true value\n",
    "#0 is for NC events\n",
    "#1 is for numu CC events\n",
    "#2 is for nue CC events\n",
    "truth=y_test[starting_number]\n",
    "if truth==0:\n",
    "    truth_stg=\"NC\"\n",
    "elif truth==1:\n",
    "    truth_stg=\"numu CC\"\n",
    "elif truth==2:\n",
    "    truth_stg=\"nue CC\"\n",
    "\n",
    "a = x_test_temp[(starting_number),:,:,0]\n",
    "\n",
    "b = x_test_temp[(starting_number),:,:,1]\n",
    "\n",
    "c = x_test_temp[(starting_number),:,:,2]\n",
    "\n",
    "\n",
    "f, axarr = plt.subplots(1,3,figsize=figsize)#, constrained_layout=True)\n",
    "f.suptitle('Event Seen From Wire Planes - 200 x 200 pixels - this is a '+truth_stg+' event')\n",
    "im=axarr[0].imshow(a, norm=matplotlib.colors.LogNorm())\n",
    "im2=axarr[1].imshow(b, norm=matplotlib.colors.LogNorm())\n",
    "im3=axarr[2].imshow(c, norm=matplotlib.colors.LogNorm())\n",
    "axarr[0].set_title('U Plane')\n",
    "axarr[1].set_title('V Plane')\n",
    "axarr[2].set_title('W Plane')\n",
    "bar1=f.colorbar(im,ax=axarr[0],fraction=0.046,pad=0.04)\n",
    "bar2=f.colorbar(im2,ax=axarr[1],fraction=0.046,pad=0.04)\n",
    "bar3=f.colorbar(im3,ax=axarr[2],fraction=0.046,pad=0.04)\n",
    "bar1.set_label('Energy [MeV]')\n",
    "bar2.set_label('Energy [MeV]')\n",
    "bar3.set_label('Energy [MeV]')\n",
    "f.tight_layout() #this is to add more space between plots\n",
    "for ax in axarr.flat:\n",
    "    ax.set(xlabel='Wire Plane', ylabel='Time')\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start.record()\n",
    "main()\n",
    "end.record()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(start.elapsed_time(end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resnet50 size 200x200 with cpu (25-02-2020) batch size 1\n",
    "# with cpu is was 3717510.25 ms, that is ~3717 s ~ 1 h for 6 epochs - final test set batch size 1\n",
    "# Test set: Average loss: 3.5087, Accuracy: 487/1000 (49%)\n",
    "\n",
    "#resnet50 size 200x200 with gpu (25-02-2020) batch size 1\n",
    "# with gpu is was 381100.1875 ms, that is ~381 s ~ 6 and a half min for 6 epochs - final test set batch size 1\n",
    "# Test set: Average loss: 2.9999, Accuracy: 499/1000 (50%)\n",
    "\n",
    "#resnet50 size 200x200 with gpu (25-02-2020) batch size 16\n",
    "# with gpu is was 329710.28125 ms, that is ~329 s ~ 5 min for 20 epochs - final test set batch size 100\n",
    "# Test set: Average loss: 1.2409, Accuracy: 664/1000 (66%)\n",
    "\n",
    "#resnet 50 11-03-2020\n",
    "# 10 epochs time 1636127.375\n",
    "# Test set: Average loss: 0.6086, Accuracy: 1553/2000 (78%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_array = []\n",
    "for i in range (1,epochs+1):\n",
    "    epochs_array.append(i)\n",
    "    \n",
    "accuracy=np.array(accuracy)\n",
    "accuracy=accuracy/100\n",
    "accuracy_train=np.array(accuracy_train)\n",
    "accuracy_train=accuracy_train/100\n",
    "epochs_array = np.array(epochs_array)\n",
    "loss = np.array(loss)\n",
    "loss_train = np.array(loss_train)\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Accuracy vs. Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.plot(epochs_array,accuracy_train,color='C1',label='Training')\n",
    "plt.plot(epochs_array,accuracy,label='Validation')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Total Loss vs. Epoch')\n",
    "plt.ylabel('Total Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.plot(epochs_array,loss_train,color='C1',label='Training')\n",
    "plt.plot(epochs_array,loss,label='Validation')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_array_total = [] # array containing all predictions for all epochs\n",
    "pred_array = [] #array containing only predictions of last epoch\n",
    "for i in range(len(pred_list)):\n",
    "    pred_array_total.append(pred_list[i].cpu().numpy())\n",
    "    \n",
    "pred_array_total=np.array(pred_array_total)\n",
    "pred_array_total=np.squeeze(np.concatenate(pred_array_total))\n",
    "#print(epochs*test_batch)\n",
    "for i in range ((epochs*(2000)-2000),(epochs*2000)):\n",
    "    pred_array.append(pred_array_total[i])\n",
    "    \n",
    "pred_array=np.array(pred_array)\n",
    "pred_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_array_total = []#array containing all the targets for all epochs\n",
    "target_array = []#array containing targets for last epoch\n",
    "for i in range(len(target_list)):\n",
    "    target_array_total.append(target_list[i].cpu().numpy())\n",
    "    \n",
    "target_array_total=np.array(target_array_total)\n",
    "target_array_total=np.squeeze(np.concatenate(target_array_total))\n",
    "\n",
    "for i in range ((epochs*2000)-2000,epochs*2000):\n",
    "    target_array.append(target_array_total[i])\n",
    "    \n",
    "target_array=np.array(target_array)\n",
    "target_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "wrong_list = []\n",
    "for i in range(len(target_array)):\n",
    "    if target_array[i]!=pred_array[i]:\n",
    "        wrong_list.append(i)\n",
    "        \n",
    "element=random.choice(wrong_list)\n",
    "\n",
    "####this part is for the true value\n",
    "#0 is for NC events\n",
    "#1 is for numu CC events\n",
    "#2 is for nue CC events\n",
    "truth_check=y_test[element]\n",
    "if truth_check==0:\n",
    "    truth_stg=\"NC\"\n",
    "elif truth_check==1:\n",
    "    truth_stg=\"numu CC\"\n",
    "elif truth_check==2:\n",
    "    truth_stg=\"nue CC\"\n",
    "    \n",
    "if pred_array[element]==0:\n",
    "    pred_stg=\"NC\"\n",
    "elif pred_array[element]==1:\n",
    "    pred_stg=\"numu CC\"\n",
    "elif pred_array[element]==2:\n",
    "    pred_stg=\"nue CC\"\n",
    "\n",
    "#print('This was supposed to be ',target_array[element],' but it was predicted as ',pred_array[element])\n",
    "print('This was supposed to be ',truth_stg,' but it was predicted as ',pred_stg)\n",
    "\n",
    "a = x_test_temp[(element),:,:,0]\n",
    "\n",
    "b = x_test_temp[(element),:,:,1]\n",
    "\n",
    "c = x_test_temp[(element),:,:,2]\n",
    "\n",
    "\n",
    "f, axarr = plt.subplots(1,3,figsize=figsize)#, constrained_layout=True)\n",
    "f.suptitle('Event Seen From Wire Planes - 200 x 200 pixels - this is a '+truth_stg+' event')\n",
    "im=axarr[0].imshow(a, norm=matplotlib.colors.LogNorm())\n",
    "im2=axarr[1].imshow(b, norm=matplotlib.colors.LogNorm())\n",
    "im3=axarr[2].imshow(c, norm=matplotlib.colors.LogNorm())\n",
    "axarr[0].set_title('U Plane')\n",
    "axarr[1].set_title('V Plane')\n",
    "axarr[2].set_title('W Plane')\n",
    "bar1=f.colorbar(im,ax=axarr[0],fraction=0.046,pad=0.04)\n",
    "bar2=f.colorbar(im2,ax=axarr[1],fraction=0.046,pad=0.04)\n",
    "bar3=f.colorbar(im3,ax=axarr[2],fraction=0.046,pad=0.04)\n",
    "bar1.set_label('Energy [MeV]')\n",
    "bar2.set_label('Energy [MeV]')\n",
    "bar3.set_label('Energy [MeV]')\n",
    "f.tight_layout() #this is to add more space between plots\n",
    "for ax in axarr.flat:\n",
    "    ax.set(xlabel='Wire Plane', ylabel='Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "torch.set_printoptions(linewidth = 120)\n",
    "import sklearn\n",
    "#print(sklearn.__version__)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "\n",
    "plt.figure(figsize=(50,50))\n",
    "cm=sklearn.metrics.confusion_matrix(target_array, pred_array)#,normalize='all')#, classes=label_dict,\n",
    "                     #title='Confusion matrix')\n",
    "cm\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "df_cm = pd.DataFrame(cm, index = [i for i in \"012\"],\n",
    "                  columns = [i for i in \"012\"])\n",
    "plt.figure(figsize = (14,10))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
